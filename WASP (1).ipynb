{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f2ef03-a0b9-4294-97e5-b27a4af27149",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T12:35:19.624746Z",
          "iopub.status.busy": "2024-08-05T12:35:19.623399Z",
          "iopub.status.idle": "2024-08-05T12:35:33.535071Z",
          "shell.execute_reply": "2024-08-05T12:35:33.533891Z",
          "shell.execute_reply.started": "2024-08-05T12:35:19.624691Z"
        },
        "tags": [],
        "id": "f5f2ef03-a0b9-4294-97e5-b27a4af27149",
        "outputId": "1d8e6aba-af9d-4189-ea5c-c32837adf7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (4.43.3)\n",
            "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.10/site-packages (2.20.0)\n",
            "Requirement already satisfied: trl in /home/jupyter/.local/lib/python3.10/site-packages (0.9.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.24.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.9.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate in /home/jupyter/.local/lib/python3.10/site-packages (from trl) (0.33.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /home/jupyter/.local/lib/python3.10/site-packages (from trl) (0.8.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->trl) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->trl) (16.0.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /home/jupyter/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.4.2)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.6.4)\n",
            "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from accelerate->trl) (6.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: fsspec==2023.9.2 in /home/jupyter/.local/lib/python3.10/site-packages (2023.9.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.10/site-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.9.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.10/site-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.9.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/jupyter/.local/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets trl\n",
        "%pip install fsspec==2023.9.2\n",
        "%pip install datasets\n",
        "%pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3197ce78-2709-4f9a-84b0-4735f7dd5080",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T12:35:14.681614Z",
          "iopub.status.busy": "2024-08-05T12:35:14.679781Z",
          "iopub.status.idle": "2024-08-05T12:35:19.621530Z",
          "shell.execute_reply": "2024-08-05T12:35:19.620210Z",
          "shell.execute_reply.started": "2024-08-05T12:35:14.681555Z"
        },
        "tags": [],
        "id": "3197ce78-2709-4f9a-84b0-4735f7dd5080",
        "outputId": "cfb7661b-78b5-4010-8ed3-2f97d67ed539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of comment pairs: 1000000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from itertools import product\n",
        "\n",
        "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
        "df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])\n",
        "positive_comments = df[df['label'] == 1]['text'].tolist()\n",
        "negative_comments = df[df['label'] == 0]['text'].tolist()\n",
        "\n",
        "num_samples = 1000\n",
        "sampled_positive_comments = random.sample(positive_comments, num_samples)\n",
        "sampled_negative_comments = random.sample(negative_comments, num_samples)\n",
        "\n",
        "comment_pairs = list(product(sampled_positive_comments, sampled_negative_comments))\n",
        "print(f\"Number of comment pairs: {len(comment_pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee0c2ae-3c7d-4bde-9a92-df13f42e22d1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T12:35:33.537794Z",
          "iopub.status.busy": "2024-08-05T12:35:33.536842Z",
          "iopub.status.idle": "2024-08-05T12:50:17.971404Z",
          "shell.execute_reply": "2024-08-05T12:50:17.970158Z",
          "shell.execute_reply.started": "2024-08-05T12:35:33.537753Z"
        },
        "tags": [],
        "id": "dee0c2ae-3c7d-4bde-9a92-df13f42e22d1",
        "outputId": "ed7303ca-4f21-40cd-dffa-8e79c4aa2c49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 1000000/1000000 [14:14<00:00, 1170.57 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
        "\n",
        "pairs_df = pd.DataFrame(comment_pairs, columns=['positive_comment', 'negative_comment'])\n",
        "dataset = Dataset.from_pandas(pairs_df)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    chosen_inputs = tokenizer(examples['positive_comment'], truncation=True, padding='max_length', max_length=512)\n",
        "    rejected_inputs = tokenizer(examples['negative_comment'], truncation=True, padding='max_length', max_length=512)\n",
        "    return {\n",
        "        'input_ids_chosen': chosen_inputs['input_ids'],\n",
        "        'attention_mask_chosen': chosen_inputs['attention_mask'],\n",
        "        'input_ids_rejected': rejected_inputs['input_ids'],\n",
        "        'attention_mask_rejected': rejected_inputs['attention_mask']\n",
        "    }\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "train_dataset = tokenized_dataset.shuffle().select(range(1000))\n",
        "\n",
        "for column in ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected']:\n",
        "    assert column in train_dataset.features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a789e64-3f91-465a-9eca-74206f5af9ed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T12:53:46.544961Z",
          "iopub.status.busy": "2024-08-05T12:53:46.543641Z",
          "iopub.status.idle": "2024-08-05T12:54:27.542164Z",
          "shell.execute_reply": "2024-08-05T12:54:27.540993Z",
          "shell.execute_reply.started": "2024-08-05T12:53:46.544909Z"
        },
        "tags": [],
        "id": "0a789e64-3f91-465a-9eca-74206f5af9ed",
        "outputId": "da1567d4-91b0-46e0-b60a-07c9b9388634"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "  0%|          | 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/125 [00:00<01:59,  1.04it/s]\u001b[A\n",
            "  2%|▏         | 2/125 [00:01<01:07,  1.82it/s]\u001b[A\n",
            "  2%|▏         | 3/125 [00:01<00:50,  2.40it/s]\u001b[A\n",
            "  3%|▎         | 4/125 [00:01<00:42,  2.82it/s]\u001b[A\n",
            "  4%|▍         | 5/125 [00:02<00:38,  3.12it/s]\u001b[A\n",
            "  5%|▍         | 6/125 [00:02<00:35,  3.34it/s]\u001b[A\n",
            "  6%|▌         | 7/125 [00:02<00:34,  3.44it/s]\u001b[A\n",
            "  6%|▋         | 8/125 [00:02<00:32,  3.55it/s]\u001b[A\n",
            "  7%|▋         | 9/125 [00:03<00:31,  3.64it/s]\u001b[A\n",
            "  8%|▊         | 10/125 [00:03<00:31,  3.69it/s]\u001b[A\n",
            "  9%|▉         | 11/125 [00:03<00:30,  3.74it/s]\u001b[A\n",
            " 10%|▉         | 12/125 [00:03<00:29,  3.77it/s]\u001b[A\n",
            " 10%|█         | 13/125 [00:04<00:29,  3.79it/s]\u001b[A\n",
            " 11%|█         | 14/125 [00:04<00:29,  3.80it/s]\u001b[A\n",
            " 12%|█▏        | 15/125 [00:04<00:28,  3.80it/s]\u001b[A\n",
            " 13%|█▎        | 16/125 [00:04<00:28,  3.81it/s]\u001b[A\n",
            " 14%|█▎        | 17/125 [00:05<00:28,  3.82it/s]\u001b[A\n",
            " 14%|█▍        | 18/125 [00:05<00:27,  3.82it/s]\u001b[A\n",
            " 15%|█▌        | 19/125 [00:05<00:27,  3.83it/s]\u001b[A\n",
            " 16%|█▌        | 20/125 [00:05<00:27,  3.84it/s]\u001b[A\n",
            " 17%|█▋        | 21/125 [00:06<00:27,  3.82it/s]\u001b[A\n",
            " 18%|█▊        | 22/125 [00:06<00:26,  3.82it/s]\u001b[A\n",
            " 18%|█▊        | 23/125 [00:06<00:26,  3.83it/s]\u001b[A\n",
            " 19%|█▉        | 24/125 [00:06<00:26,  3.83it/s]\u001b[A\n",
            " 20%|██        | 25/125 [00:07<00:26,  3.83it/s]\u001b[A\n",
            " 21%|██        | 26/125 [00:07<00:25,  3.83it/s]\u001b[A\n",
            " 22%|██▏       | 27/125 [00:07<00:25,  3.83it/s]\u001b[A\n",
            " 22%|██▏       | 28/125 [00:08<00:25,  3.82it/s]\u001b[A\n",
            " 23%|██▎       | 29/125 [00:08<00:25,  3.83it/s]\u001b[A\n",
            " 24%|██▍       | 30/125 [00:08<00:24,  3.83it/s]\u001b[A\n",
            " 25%|██▍       | 31/125 [00:08<00:24,  3.83it/s]\u001b[A\n",
            " 26%|██▌       | 32/125 [00:09<00:24,  3.83it/s]\u001b[A\n",
            " 26%|██▋       | 33/125 [00:09<00:24,  3.83it/s]\u001b[A\n",
            " 27%|██▋       | 34/125 [00:09<00:23,  3.84it/s]\u001b[A\n",
            " 28%|██▊       | 35/125 [00:09<00:23,  3.84it/s]\u001b[A\n",
            " 29%|██▉       | 36/125 [00:10<00:23,  3.84it/s]\u001b[A\n",
            " 30%|██▉       | 37/125 [00:10<00:22,  3.84it/s]\u001b[A\n",
            " 30%|███       | 38/125 [00:10<00:22,  3.79it/s]\u001b[A\n",
            " 31%|███       | 39/125 [00:10<00:22,  3.81it/s]\u001b[A\n",
            " 32%|███▏      | 40/125 [00:11<00:22,  3.81it/s]\u001b[A\n",
            " 33%|███▎      | 41/125 [00:11<00:21,  3.82it/s]\u001b[A\n",
            " 34%|███▎      | 42/125 [00:11<00:21,  3.82it/s]\u001b[A\n",
            " 34%|███▍      | 43/125 [00:11<00:21,  3.83it/s]\u001b[A\n",
            " 35%|███▌      | 44/125 [00:12<00:21,  3.84it/s]\u001b[A\n",
            " 36%|███▌      | 45/125 [00:12<00:20,  3.84it/s]\u001b[A\n",
            " 37%|███▋      | 46/125 [00:12<00:20,  3.83it/s]\u001b[A\n",
            " 38%|███▊      | 47/125 [00:12<00:20,  3.83it/s]\u001b[A\n",
            " 38%|███▊      | 48/125 [00:13<00:20,  3.84it/s]\u001b[A\n",
            " 39%|███▉      | 49/125 [00:13<00:19,  3.84it/s]\u001b[A\n",
            " 40%|████      | 50/125 [00:13<00:19,  3.85it/s]\u001b[A\n",
            " 41%|████      | 51/125 [00:14<00:19,  3.84it/s]\u001b[A\n",
            " 42%|████▏     | 52/125 [00:14<00:19,  3.84it/s]\u001b[A\n",
            " 42%|████▏     | 53/125 [00:14<00:18,  3.83it/s]\u001b[A\n",
            " 43%|████▎     | 54/125 [00:14<00:18,  3.83it/s]\u001b[A\n",
            " 44%|████▍     | 55/125 [00:15<00:18,  3.84it/s]\u001b[A\n",
            " 45%|████▍     | 56/125 [00:15<00:17,  3.84it/s]\u001b[A\n",
            " 46%|████▌     | 57/125 [00:15<00:17,  3.84it/s]\u001b[A\n",
            " 46%|████▋     | 58/125 [00:15<00:17,  3.84it/s]\u001b[A\n",
            " 47%|████▋     | 59/125 [00:16<00:17,  3.84it/s]\u001b[A\n",
            " 48%|████▊     | 60/125 [00:16<00:16,  3.84it/s]\u001b[A\n",
            " 49%|████▉     | 61/125 [00:16<00:16,  3.83it/s]\u001b[A\n",
            " 50%|████▉     | 62/125 [00:16<00:16,  3.82it/s]\u001b[A\n",
            " 50%|█████     | 63/125 [00:17<00:16,  3.83it/s]\u001b[A\n",
            " 51%|█████     | 64/125 [00:17<00:15,  3.84it/s]\u001b[A\n",
            " 52%|█████▏    | 65/125 [00:17<00:15,  3.84it/s]\u001b[A\n",
            " 53%|█████▎    | 66/125 [00:17<00:15,  3.84it/s]\u001b[A\n",
            " 54%|█████▎    | 67/125 [00:18<00:15,  3.83it/s]\u001b[A\n",
            " 54%|█████▍    | 68/125 [00:18<00:14,  3.83it/s]\u001b[A\n",
            " 55%|█████▌    | 69/125 [00:18<00:14,  3.83it/s]\u001b[A\n",
            " 56%|█████▌    | 70/125 [00:18<00:14,  3.83it/s]\u001b[A\n",
            " 57%|█████▋    | 71/125 [00:19<00:14,  3.84it/s]\u001b[A\n",
            " 58%|█████▊    | 72/125 [00:19<00:13,  3.84it/s]\u001b[A\n",
            " 58%|█████▊    | 73/125 [00:19<00:13,  3.84it/s]\u001b[A\n",
            " 59%|█████▉    | 74/125 [00:20<00:13,  3.84it/s]\u001b[A\n",
            " 60%|██████    | 75/125 [00:20<00:13,  3.81it/s]\u001b[A\n",
            " 61%|██████    | 76/125 [00:20<00:12,  3.82it/s]\u001b[A\n",
            " 62%|██████▏   | 77/125 [00:20<00:12,  3.83it/s]\u001b[A\n",
            " 62%|██████▏   | 78/125 [00:21<00:12,  3.83it/s]\u001b[A\n",
            " 63%|██████▎   | 79/125 [00:21<00:12,  3.78it/s]\u001b[A\n",
            " 64%|██████▍   | 80/125 [00:21<00:11,  3.80it/s]\u001b[A\n",
            " 65%|██████▍   | 81/125 [00:21<00:11,  3.80it/s]\u001b[A\n",
            " 66%|██████▌   | 82/125 [00:22<00:11,  3.81it/s]\u001b[A\n",
            " 66%|██████▋   | 83/125 [00:22<00:10,  3.82it/s]\u001b[A\n",
            " 67%|██████▋   | 84/125 [00:22<00:10,  3.82it/s]\u001b[A\n",
            " 68%|██████▊   | 85/125 [00:22<00:10,  3.82it/s]\u001b[A\n",
            " 69%|██████▉   | 86/125 [00:23<00:10,  3.83it/s]\u001b[A\n",
            " 70%|██████▉   | 87/125 [00:23<00:09,  3.83it/s]\u001b[A\n",
            " 70%|███████   | 88/125 [00:23<00:09,  3.83it/s]\u001b[A\n",
            " 71%|███████   | 89/125 [00:23<00:09,  3.83it/s]\u001b[A\n",
            " 72%|███████▏  | 90/125 [00:24<00:09,  3.83it/s]\u001b[A\n",
            " 73%|███████▎  | 91/125 [00:24<00:08,  3.83it/s]\u001b[A\n",
            " 74%|███████▎  | 92/125 [00:24<00:08,  3.83it/s]\u001b[A\n",
            " 74%|███████▍  | 93/125 [00:24<00:08,  3.83it/s]\u001b[A\n",
            " 75%|███████▌  | 94/125 [00:25<00:08,  3.83it/s]\u001b[A\n",
            " 76%|███████▌  | 95/125 [00:25<00:07,  3.83it/s]\u001b[A\n",
            " 77%|███████▋  | 96/125 [00:25<00:07,  3.83it/s]\u001b[A\n",
            " 78%|███████▊  | 97/125 [00:26<00:07,  3.84it/s]\u001b[A\n",
            " 78%|███████▊  | 98/125 [00:26<00:07,  3.84it/s]\u001b[A\n",
            " 79%|███████▉  | 99/125 [00:26<00:06,  3.83it/s]\u001b[A\n",
            " 80%|████████  | 100/125 [00:26<00:06,  3.83it/s]\u001b[A\n",
            " 81%|████████  | 101/125 [00:27<00:06,  3.83it/s]\u001b[A\n",
            " 82%|████████▏ | 102/125 [00:27<00:06,  3.83it/s]\u001b[A\n",
            " 82%|████████▏ | 103/125 [00:27<00:05,  3.83it/s]\u001b[A\n",
            " 83%|████████▎ | 104/125 [00:27<00:05,  3.83it/s]\u001b[A\n",
            " 84%|████████▍ | 105/125 [00:28<00:05,  3.82it/s]\u001b[A\n",
            " 85%|████████▍ | 106/125 [00:28<00:04,  3.82it/s]\u001b[A\n",
            " 86%|████████▌ | 107/125 [00:28<00:04,  3.82it/s]\u001b[A\n",
            " 86%|████████▋ | 108/125 [00:28<00:04,  3.82it/s]\u001b[A\n",
            " 87%|████████▋ | 109/125 [00:29<00:04,  3.83it/s]\u001b[A\n",
            " 88%|████████▊ | 110/125 [00:29<00:03,  3.83it/s]\u001b[A\n",
            " 89%|████████▉ | 111/125 [00:29<00:03,  3.82it/s]\u001b[A\n",
            " 90%|████████▉ | 112/125 [00:29<00:03,  3.83it/s]\u001b[A\n",
            " 90%|█████████ | 113/125 [00:30<00:03,  3.83it/s]\u001b[A\n",
            " 91%|█████████ | 114/125 [00:30<00:02,  3.83it/s]\u001b[A\n",
            " 92%|█████████▏| 115/125 [00:30<00:02,  3.82it/s]\u001b[A\n",
            " 93%|█████████▎| 116/125 [00:31<00:02,  3.78it/s]\u001b[A\n",
            " 94%|█████████▎| 117/125 [00:31<00:02,  3.79it/s]\u001b[A\n",
            " 94%|█████████▍| 118/125 [00:31<00:01,  3.80it/s]\u001b[A\n",
            " 95%|█████████▌| 119/125 [00:31<00:01,  3.81it/s]\u001b[A\n",
            " 96%|█████████▌| 120/125 [00:32<00:01,  3.81it/s]\u001b[A\n",
            " 97%|█████████▋| 121/125 [00:32<00:01,  3.81it/s]\u001b[A\n",
            " 98%|█████████▊| 122/125 [00:32<00:00,  3.81it/s]\u001b[A\n",
            " 98%|█████████▊| 123/125 [00:32<00:00,  3.82it/s]\u001b[A\n",
            " 99%|█████████▉| 124/125 [00:33<00:00,  3.82it/s]\u001b[A\n",
            "                                        3.87it/s]\u001b[A\n",
            "  0%|          | 0/125 [02:10<?, ?it/s]          \n",
            "100%|██████████| 125/125 [00:38<00:00,  3.24it/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 38.5608, 'train_samples_per_second': 25.933, 'train_steps_per_second': 3.242, 'train_loss': 0.1842788391113281, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./reward_model/tokenizer_config.json',\n",
              " './reward_model/special_tokens_map.json',\n",
              " './reward_model/vocab.txt',\n",
              " './reward_model/added_tokens.json',\n",
              " './reward_model/tokenizer.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=1)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "def data_collator(features):\n",
        "    batch = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'labels': []\n",
        "    }\n",
        "\n",
        "    for feature in features:\n",
        "        batch['input_ids'].append(feature['input_ids_chosen'])\n",
        "        batch['attention_mask'].append(feature['attention_mask_chosen'])\n",
        "        batch['labels'].append(1.0)\n",
        "\n",
        "        batch['input_ids'].append(feature['input_ids_rejected'])\n",
        "        batch['attention_mask'].append(feature['attention_mask_rejected'])\n",
        "        batch['labels'].append(0.0)\n",
        "\n",
        "    batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
        "\n",
        "    batch['input_ids'] = batch['input_ids'].long()\n",
        "    batch['attention_mask'] = batch['attention_mask'].long()\n",
        "    batch['labels'] = batch['labels'].float()\n",
        "\n",
        "    return batch\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained('./reward_model')\n",
        "tokenizer.save_pretrained('./reward_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4365de83-dcce-4974-850f-0d64755c4b23",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T12:54:27.545629Z",
          "iopub.status.busy": "2024-08-05T12:54:27.544106Z",
          "iopub.status.idle": "2024-08-05T12:54:27.680006Z",
          "shell.execute_reply": "2024-08-05T12:54:27.678852Z",
          "shell.execute_reply.started": "2024-08-05T12:54:27.545559Z"
        },
        "id": "4365de83-dcce-4974-850f-0d64755c4b23"
      },
      "outputs": [],
      "source": [
        "reward_model = AutoModelForSequenceClassification.from_pretrained('./reward_model')\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained('./reward_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a34a8ce-2b91-4cfe-ba4e-627cfbc0d3a1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T12:56:51.487910Z",
          "iopub.status.busy": "2024-08-05T12:56:51.486690Z",
          "iopub.status.idle": "2024-08-05T12:56:51.802615Z",
          "shell.execute_reply": "2024-08-05T12:56:51.801298Z",
          "shell.execute_reply.started": "2024-08-05T12:56:51.487872Z"
        },
        "tags": [],
        "id": "9a34a8ce-2b91-4cfe-ba4e-627cfbc0d3a1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def create_prompts(df, num_prompts=100, min_tokens=5, max_tokens=15, max_length=512):\n",
        "    prompts = []\n",
        "    for text in df['text'].sample(num_prompts):\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        if len(tokens) > min_tokens:\n",
        "            truncated_tokens = tokens[:random.randint(min_tokens, max_tokens)]\n",
        "            truncated_tokens = truncated_tokens[:max_length]  # Ensure the sequence is within the max length\n",
        "            prompts.append(tokenizer.convert_tokens_to_string(truncated_tokens))\n",
        "    return prompts\n",
        "\n",
        "train_prompts = create_prompts(df, num_prompts=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "385161bd-036c-4c37-93d4-7bff111888be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T13:00:06.667657Z",
          "iopub.status.busy": "2024-08-05T13:00:06.666420Z",
          "iopub.status.idle": "2024-08-05T13:00:10.419818Z",
          "shell.execute_reply": "2024-08-05T13:00:10.418710Z",
          "shell.execute_reply.started": "2024-08-05T13:00:06.667611Z"
        },
        "tags": [],
        "id": "385161bd-036c-4c37-93d4-7bff111888be"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('lvwerra/gpt2-imdb')\n",
        "ref_model = GPT2LMHeadModel.from_pretrained('lvwerra/gpt2-imdb')\n",
        "optimizer = AdamW(gpt2_model.parameters(), lr=1e-5)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('lvwerra/gpt2-imdb')\n",
        "\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "gpt2_model.resize_token_embeddings(len(tokenizer))\n",
        "ref_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained('./reward_model')\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained('./reward_model')\n",
        "\n",
        "theta_init = gpt2_model.state_dict()\n",
        "theta_sft = gpt2_model.state_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3df95d-8f71-4def-9980-6745fee6548e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T13:00:21.051351Z",
          "iopub.status.busy": "2024-08-05T13:00:21.050663Z",
          "iopub.status.idle": "2024-08-05T13:00:21.073881Z",
          "shell.execute_reply": "2024-08-05T13:00:21.072661Z",
          "shell.execute_reply.started": "2024-08-05T13:00:21.051316Z"
        },
        "id": "7a3df95d-8f71-4def-9980-6745fee6548e"
      },
      "outputs": [],
      "source": [
        "def compute_reward(prompt, generated_text):\n",
        "    prompt_inputs = reward_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    generated_inputs = reward_tokenizer(generated_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_output = reward_model(**generated_inputs)\n",
        "\n",
        "    reward = torch.sigmoid(generated_output.logits).mean().item()\n",
        "    return reward\n",
        "\n",
        "def slurp(theta_init, thetas, lambd):\n",
        "    result = {}\n",
        "    for key in theta_init.keys():\n",
        "        result[key] = (1 - lambd) * theta_init[key]\n",
        "        for theta in thetas:\n",
        "            result[key] += (lambd / len(thetas)) * theta[key]\n",
        "    return result\n",
        "\n",
        "def evaluate_kl_divergence(output, ref_model_output):\n",
        "    kl_divergence = F.kl_div(output.log_softmax(dim=-1), ref_model_output.softmax(dim=-1), reduction='batchmean')\n",
        "    return kl_divergence.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f0a344-c3fc-474d-832b-f0224a7b1321",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T13:00:24.923374Z",
          "iopub.status.busy": "2024-08-05T13:00:24.921945Z",
          "iopub.status.idle": "2024-08-05T13:00:25.278717Z",
          "shell.execute_reply": "2024-08-05T13:00:25.277595Z",
          "shell.execute_reply.started": "2024-08-05T13:00:24.923300Z"
        },
        "id": "80f0a344-c3fc-474d-832b-f0224a7b1321"
      },
      "outputs": [],
      "source": [
        "def create_prompts(df, num_prompts=100, min_tokens=5, max_tokens=15, max_length=512):\n",
        "    prompts = []\n",
        "    for text in df['text'].sample(num_prompts):\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        if len(tokens) > min_tokens:\n",
        "            truncated_tokens = tokens[:random.randint(min_tokens, max_tokens)]\n",
        "            truncated_tokens = truncated_tokens[:max_length]\n",
        "            prompts.append(tokenizer.convert_tokens_to_string(truncated_tokens))\n",
        "    return prompts\n",
        "\n",
        "train_prompts = create_prompts(df, num_prompts=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00917e1f-9000-4c9a-a872-38a8b5b2c48d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T13:00:32.270317Z",
          "iopub.status.busy": "2024-08-05T13:00:32.269309Z",
          "iopub.status.idle": "2024-08-05T15:05:04.722203Z"
        },
        "tags": [],
        "id": "00917e1f-9000-4c9a-a872-38a8b5b2c48d",
        "outputId": "7ab929bc-e030-4982-9bd7-375804933b41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterations:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Models:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Training Steps:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   1%|          | 1/100 [00:19<32:23, 19.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   2%|▏         | 2/100 [00:39<31:58, 19.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   3%|▎         | 3/100 [00:58<31:24, 19.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   4%|▍         | 4/100 [01:17<30:53, 19.31s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   5%|▌         | 5/100 [01:37<30:42, 19.39s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   6%|▌         | 6/100 [01:56<30:15, 19.32s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   7%|▋         | 7/100 [02:15<29:45, 19.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   8%|▊         | 8/100 [02:34<29:14, 19.07s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   9%|▉         | 9/100 [02:52<28:40, 18.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  10%|█         | 10/100 [03:11<28:16, 18.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  11%|█         | 11/100 [03:29<27:53, 18.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  12%|█▏        | 12/100 [03:48<27:20, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  13%|█▎        | 13/100 [04:06<26:59, 18.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  14%|█▍        | 14/100 [04:25<26:36, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  15%|█▌        | 15/100 [04:43<26:09, 18.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  16%|█▌        | 16/100 [05:02<25:57, 18.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  17%|█▋        | 17/100 [05:20<25:42, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  18%|█▊        | 18/100 [05:39<25:22, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  19%|█▉        | 19/100 [05:58<25:08, 18.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  20%|██        | 20/100 [06:16<24:50, 18.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  21%|██        | 21/100 [06:35<24:38, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  22%|██▏       | 22/100 [06:54<24:16, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  23%|██▎       | 23/100 [07:12<23:57, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  24%|██▍       | 24/100 [07:31<23:40, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  25%|██▌       | 25/100 [07:50<23:21, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  26%|██▌       | 26/100 [08:09<23:01, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  27%|██▋       | 27/100 [08:27<22:36, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  28%|██▊       | 28/100 [08:45<22:11, 18.50s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  29%|██▉       | 29/100 [09:04<21:55, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  30%|███       | 30/100 [09:22<21:38, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  31%|███       | 31/100 [09:41<21:19, 18.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  32%|███▏      | 32/100 [10:00<21:03, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  33%|███▎      | 33/100 [10:18<20:48, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  34%|███▍      | 34/100 [10:37<20:33, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  35%|███▌      | 35/100 [10:56<20:14, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  36%|███▌      | 36/100 [11:14<19:52, 18.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  37%|███▋      | 37/100 [11:33<19:28, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  38%|███▊      | 38/100 [11:51<19:04, 18.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  39%|███▉      | 39/100 [12:09<18:45, 18.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  40%|████      | 40/100 [12:28<18:24, 18.41s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  41%|████      | 41/100 [12:46<18:04, 18.38s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  42%|████▏     | 42/100 [13:04<17:47, 18.41s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  43%|████▎     | 43/100 [13:23<17:33, 18.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  44%|████▍     | 44/100 [13:42<17:16, 18.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  45%|████▌     | 45/100 [14:01<17:02, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  46%|████▌     | 46/100 [14:19<16:42, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  47%|████▋     | 47/100 [14:37<16:22, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  48%|████▊     | 48/100 [14:56<16:03, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  49%|████▉     | 49/100 [15:15<15:46, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  50%|█████     | 50/100 [15:33<15:27, 18.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  51%|█████     | 51/100 [15:51<15:05, 18.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  52%|█████▏    | 52/100 [16:10<14:47, 18.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  53%|█████▎    | 53/100 [16:29<14:30, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  54%|█████▍    | 54/100 [16:47<14:11, 18.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  55%|█████▌    | 55/100 [17:06<13:54, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  56%|█████▌    | 56/100 [17:24<13:35, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  57%|█████▋    | 57/100 [17:43<13:14, 18.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  58%|█████▊    | 58/100 [18:01<12:59, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  59%|█████▉    | 59/100 [18:20<12:40, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  60%|██████    | 60/100 [18:39<12:24, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  61%|██████    | 61/100 [18:57<12:07, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  62%|██████▏   | 62/100 [19:16<11:49, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  63%|██████▎   | 63/100 [19:35<11:30, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  64%|██████▍   | 64/100 [19:53<11:08, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  65%|██████▌   | 65/100 [20:11<10:48, 18.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  66%|██████▌   | 66/100 [20:30<10:29, 18.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  67%|██████▋   | 67/100 [20:48<10:11, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  68%|██████▊   | 68/100 [21:07<09:50, 18.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  69%|██████▉   | 69/100 [21:25<09:32, 18.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  72%|███████▏  | 72/100 [22:21<08:36, 18.44s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  73%|███████▎  | 73/100 [22:39<08:19, 18.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  74%|███████▍  | 74/100 [22:58<08:02, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  75%|███████▌  | 75/100 [23:17<07:44, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  76%|███████▌  | 76/100 [23:35<07:27, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  77%|███████▋  | 77/100 [23:54<07:08, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  78%|███████▊  | 78/100 [24:13<06:50, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  79%|███████▉  | 79/100 [24:31<06:30, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  80%|████████  | 80/100 [24:50<06:11, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  81%|████████  | 81/100 [25:08<05:52, 18.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  82%|████████▏ | 82/100 [25:27<05:34, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  83%|████████▎ | 83/100 [25:45<05:16, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  84%|████████▍ | 84/100 [26:04<04:57, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  85%|████████▌ | 85/100 [26:23<04:38, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  86%|████████▌ | 86/100 [26:41<04:19, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  87%|████████▋ | 87/100 [27:00<04:01, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  88%|████████▊ | 88/100 [27:18<03:42, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  89%|████████▉ | 89/100 [27:37<03:24, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  90%|█████████ | 90/100 [27:56<03:06, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  91%|█████████ | 91/100 [28:14<02:47, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  92%|█████████▏| 92/100 [28:33<02:29, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  93%|█████████▎| 93/100 [28:51<02:10, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  94%|█████████▍| 94/100 [29:10<01:51, 18.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  95%|█████████▌| 95/100 [29:29<01:32, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  96%|█████████▌| 96/100 [29:47<01:14, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  97%|█████████▋| 97/100 [30:06<00:55, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  98%|█████████▊| 98/100 [30:24<00:37, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  99%|█████████▉| 99/100 [30:43<00:18, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps: 100%|██████████| 100/100 [31:01<00:00, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Models:  50%|█████     | 1/2 [31:02<31:02, 1862.09s/it]\u001b[A\n",
            "\n",
            "Training Steps:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   1%|          | 1/100 [00:18<30:43, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   2%|▏         | 2/100 [00:37<30:23, 18.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   3%|▎         | 3/100 [00:55<30:06, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   4%|▍         | 4/100 [01:14<29:41, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   5%|▌         | 5/100 [01:33<29:31, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   6%|▌         | 6/100 [01:51<29:06, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   7%|▋         | 7/100 [02:10<28:49, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   8%|▊         | 8/100 [02:28<28:31, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   9%|▉         | 9/100 [02:47<28:18, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  10%|█         | 10/100 [03:06<27:59, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  11%|█         | 11/100 [03:24<27:42, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  12%|█▏        | 12/100 [03:43<27:24, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  13%|█▎        | 13/100 [04:02<27:03, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  14%|█▍        | 14/100 [04:21<26:47, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  15%|█▌        | 15/100 [04:39<26:29, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  16%|█▌        | 16/100 [04:58<26:07, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  17%|█▋        | 17/100 [05:17<25:51, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  18%|█▊        | 18/100 [05:35<25:29, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  19%|█▉        | 19/100 [05:54<25:04, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  20%|██        | 20/100 [06:12<24:47, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  21%|██        | 21/100 [06:31<24:28, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  22%|██▏       | 22/100 [06:49<24:07, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  23%|██▎       | 23/100 [07:08<23:51, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  24%|██▍       | 24/100 [07:27<23:37, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  25%|██▌       | 25/100 [07:45<23:16, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  26%|██▌       | 26/100 [08:04<23:03, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  27%|██▋       | 27/100 [08:23<22:45, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  28%|██▊       | 28/100 [08:41<22:25, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  29%|██▉       | 29/100 [09:00<22:08, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  30%|███       | 30/100 [09:19<21:51, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  31%|███       | 31/100 [09:38<21:31, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  32%|███▏      | 32/100 [09:56<21:12, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  33%|███▎      | 33/100 [10:15<20:52, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  34%|███▍      | 34/100 [10:34<20:36, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  35%|███▌      | 35/100 [10:53<20:20, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  36%|███▌      | 36/100 [11:11<19:55, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  37%|███▋      | 37/100 [11:30<19:32, 18.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  38%|███▊      | 38/100 [11:48<19:12, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  39%|███▉      | 39/100 [12:07<18:57, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  40%|████      | 40/100 [12:26<18:36, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  41%|████      | 41/100 [12:44<18:20, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  42%|████▏     | 42/100 [13:03<18:01, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  43%|████▎     | 43/100 [13:22<17:44, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  44%|████▍     | 44/100 [13:40<17:23, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  45%|████▌     | 45/100 [13:59<17:04, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  46%|████▌     | 46/100 [14:17<16:44, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  47%|████▋     | 47/100 [14:36<16:24, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  48%|████▊     | 48/100 [14:54<16:06, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  49%|████▉     | 49/100 [15:13<15:49, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  50%|█████     | 50/100 [15:32<15:28, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  51%|█████     | 51/100 [15:50<15:12, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  52%|█████▏    | 52/100 [16:09<14:57, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  53%|█████▎    | 53/100 [16:28<14:38, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  54%|█████▍    | 54/100 [16:46<14:16, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  55%|█████▌    | 55/100 [17:05<13:54, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  56%|█████▌    | 56/100 [17:23<13:35, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  57%|█████▋    | 57/100 [17:42<13:15, 18.50s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  58%|█████▊    | 58/100 [18:00<12:55, 18.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  59%|█████▉    | 59/100 [18:18<12:36, 18.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  60%|██████    | 60/100 [18:37<12:17, 18.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  61%|██████    | 61/100 [18:55<12:00, 18.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  62%|██████▏   | 62/100 [19:14<11:41, 18.47s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  63%|██████▎   | 63/100 [19:32<11:19, 18.38s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  64%|██████▍   | 64/100 [19:51<11:03, 18.44s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  65%|██████▌   | 65/100 [20:09<10:47, 18.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  66%|██████▌   | 66/100 [20:28<10:29, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  67%|██████▋   | 67/100 [20:47<10:12, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  68%|██████▊   | 68/100 [21:05<09:56, 18.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  69%|██████▉   | 69/100 [21:24<09:38, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  70%|███████   | 70/100 [21:42<09:17, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  71%|███████   | 71/100 [22:01<08:59, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  72%|███████▏  | 72/100 [22:20<08:40, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  73%|███████▎  | 73/100 [22:38<08:21, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  74%|███████▍  | 74/100 [22:57<08:03, 18.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  75%|███████▌  | 75/100 [23:16<07:46, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  76%|███████▌  | 76/100 [23:34<07:27, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  77%|███████▋  | 77/100 [23:53<07:09, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  78%|███████▊  | 78/100 [24:12<06:51, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  79%|███████▉  | 79/100 [24:31<06:33, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  80%|████████  | 80/100 [24:49<06:14, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  81%|████████  | 81/100 [25:08<05:55, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  82%|████████▏ | 82/100 [25:27<05:36, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  83%|████████▎ | 83/100 [25:45<05:18, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  84%|████████▍ | 84/100 [26:04<04:58, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  85%|████████▌ | 85/100 [26:22<04:38, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  86%|████████▌ | 86/100 [26:41<04:20, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  87%|████████▋ | 87/100 [27:00<04:02, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  88%|████████▊ | 88/100 [27:18<03:42, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  89%|████████▉ | 89/100 [27:37<03:24, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  90%|█████████ | 90/100 [27:55<03:05, 18.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  91%|█████████ | 91/100 [28:14<02:47, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  92%|█████████▏| 92/100 [28:32<02:28, 18.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  93%|█████████▎| 93/100 [28:51<02:09, 18.50s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  94%|█████████▍| 94/100 [29:09<01:51, 18.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  95%|█████████▌| 95/100 [29:28<01:32, 18.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  96%|█████████▌| 96/100 [29:47<01:14, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  97%|█████████▋| 97/100 [30:05<00:55, 18.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  98%|█████████▊| 98/100 [30:24<00:37, 18.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  99%|█████████▉| 99/100 [30:42<00:18, 18.50s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps: 100%|██████████| 100/100 [31:00<00:00, 18.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Models: 100%|██████████| 2/2 [1:02:03<00:00, 1861.57s/it]\u001b[A\n",
            "Iterations:  50%|█████     | 1/2 [1:02:03<1:02:03, 3723.65s/it]\n",
            "Models:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Training Steps:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   1%|          | 1/100 [00:18<30:59, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   2%|▏         | 2/100 [00:37<30:48, 18.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   3%|▎         | 3/100 [00:56<30:25, 18.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   4%|▍         | 4/100 [01:15<30:07, 18.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   5%|▌         | 5/100 [01:34<29:47, 18.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   6%|▌         | 6/100 [01:52<29:24, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   7%|▋         | 7/100 [02:11<29:04, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   8%|▊         | 8/100 [02:30<28:51, 18.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   9%|▉         | 9/100 [02:49<28:30, 18.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  10%|█         | 10/100 [03:07<28:07, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  11%|█         | 11/100 [03:26<27:52, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  12%|█▏        | 12/100 [03:45<27:28, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  13%|█▎        | 13/100 [04:04<27:12, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  14%|█▍        | 14/100 [04:23<26:55, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  15%|█▌        | 15/100 [04:41<26:34, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  16%|█▌        | 16/100 [05:00<26:16, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  17%|█▋        | 17/100 [05:19<25:53, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  18%|█▊        | 18/100 [05:37<25:30, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  19%|█▉        | 19/100 [05:56<25:13, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  20%|██        | 20/100 [06:15<24:57, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  21%|██        | 21/100 [06:33<24:40, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  22%|██▏       | 22/100 [06:52<24:21, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  23%|██▎       | 23/100 [07:11<24:05, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  24%|██▍       | 24/100 [07:30<23:47, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  25%|██▌       | 25/100 [07:48<23:23, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  26%|██▌       | 26/100 [08:07<22:59, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  27%|██▋       | 27/100 [08:26<22:40, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  28%|██▊       | 28/100 [08:44<22:24, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  29%|██▉       | 29/100 [09:03<22:06, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  30%|███       | 30/100 [09:22<21:46, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  31%|███       | 31/100 [09:40<21:29, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  32%|███▏      | 32/100 [09:59<21:11, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  33%|███▎      | 33/100 [10:18<20:52, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  34%|███▍      | 34/100 [10:37<20:34, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  35%|███▌      | 35/100 [10:55<20:15, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  36%|███▌      | 36/100 [11:14<19:59, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  37%|███▋      | 37/100 [11:33<19:40, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  38%|███▊      | 38/100 [11:52<19:24, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  39%|███▉      | 39/100 [12:10<19:05, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  40%|████      | 40/100 [12:29<18:43, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  41%|████      | 41/100 [12:48<18:21, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  42%|████▏     | 42/100 [13:06<18:03, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  43%|████▎     | 43/100 [13:25<17:43, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  44%|████▍     | 44/100 [13:44<17:26, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  45%|████▌     | 45/100 [14:02<17:05, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  46%|████▌     | 46/100 [14:21<16:45, 18.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  47%|████▋     | 47/100 [14:39<16:25, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  48%|████▊     | 48/100 [14:58<16:10, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  49%|████▉     | 49/100 [15:17<15:54, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  50%|█████     | 50/100 [15:36<15:36, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  51%|█████     | 51/100 [15:55<15:19, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  52%|█████▏    | 52/100 [16:13<15:00, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  53%|█████▎    | 53/100 [16:32<14:42, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  54%|█████▍    | 54/100 [16:51<14:24, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  55%|█████▌    | 55/100 [17:10<14:05, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  56%|█████▌    | 56/100 [17:29<13:47, 18.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  57%|█████▋    | 57/100 [17:47<13:29, 18.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  58%|█████▊    | 58/100 [18:06<13:07, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  59%|█████▉    | 59/100 [18:25<12:50, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  60%|██████    | 60/100 [18:44<12:31, 18.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  61%|██████    | 61/100 [19:02<12:11, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  62%|██████▏   | 62/100 [19:21<11:51, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  63%|██████▎   | 63/100 [19:40<11:33, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  66%|██████▌   | 66/100 [20:36<10:34, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  67%|██████▋   | 67/100 [20:54<10:16, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  68%|██████▊   | 68/100 [21:13<09:58, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  69%|██████▉   | 69/100 [21:32<09:40, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  70%|███████   | 70/100 [21:51<09:23, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  71%|███████   | 71/100 [22:10<09:05, 18.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  72%|███████▏  | 72/100 [22:28<08:45, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  73%|███████▎  | 73/100 [22:47<08:24, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  74%|███████▍  | 74/100 [23:06<08:07, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  75%|███████▌  | 75/100 [23:25<07:48, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  76%|███████▌  | 76/100 [23:43<07:29, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  77%|███████▋  | 77/100 [24:02<07:10, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  78%|███████▊  | 78/100 [24:21<06:51, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  79%|███████▉  | 79/100 [24:39<06:33, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  80%|████████  | 80/100 [24:58<06:14, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  81%|████████  | 81/100 [25:17<05:54, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  82%|████████▏ | 82/100 [25:36<05:37, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  83%|████████▎ | 83/100 [25:54<05:18, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  84%|████████▍ | 84/100 [26:13<04:59, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  85%|████████▌ | 85/100 [26:32<04:41, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  86%|████████▌ | 86/100 [26:50<04:21, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  87%|████████▋ | 87/100 [27:09<04:03, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  88%|████████▊ | 88/100 [27:28<03:44, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  89%|████████▉ | 89/100 [27:47<03:26, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  90%|█████████ | 90/100 [28:05<03:07, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  91%|█████████ | 91/100 [28:24<02:48, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  92%|█████████▏| 92/100 [28:42<02:29, 18.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  93%|█████████▎| 93/100 [29:01<02:10, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  94%|█████████▍| 94/100 [29:20<01:51, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  95%|█████████▌| 95/100 [29:38<01:33, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  96%|█████████▌| 96/100 [29:57<01:14, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  97%|█████████▋| 97/100 [30:16<00:56, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  98%|█████████▊| 98/100 [30:35<00:37, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  99%|█████████▉| 99/100 [30:54<00:18, 18.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps: 100%|██████████| 100/100 [31:13<00:00, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Models:  50%|█████     | 1/2 [31:13<31:13, 1873.16s/it]\u001b[A\n",
            "\n",
            "Training Steps:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   1%|          | 1/100 [00:18<31:11, 18.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   2%|▏         | 2/100 [00:37<30:40, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   5%|▌         | 5/100 [01:34<29:51, 18.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   6%|▌         | 6/100 [01:53<29:31, 18.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   7%|▋         | 7/100 [02:11<29:14, 18.87s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   8%|▊         | 8/100 [02:30<28:54, 18.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:   9%|▉         | 9/100 [02:49<28:34, 18.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  10%|█         | 10/100 [03:08<28:16, 18.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  11%|█         | 11/100 [03:26<27:45, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  12%|█▏        | 12/100 [03:45<27:27, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  13%|█▎        | 13/100 [04:04<27:06, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  14%|█▍        | 14/100 [04:22<26:45, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  15%|█▌        | 15/100 [04:41<26:20, 18.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  16%|█▌        | 16/100 [04:59<26:05, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  17%|█▋        | 17/100 [05:18<25:51, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  18%|█▊        | 18/100 [05:37<25:36, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  19%|█▉        | 19/100 [05:56<25:20, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  20%|██        | 20/100 [06:15<25:04, 18.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  21%|██        | 21/100 [06:33<24:39, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  22%|██▏       | 22/100 [06:52<24:20, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  23%|██▎       | 23/100 [07:11<24:01, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  24%|██▍       | 24/100 [07:30<23:45, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  25%|██▌       | 25/100 [07:48<23:25, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  26%|██▌       | 26/100 [08:07<23:07, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  27%|██▋       | 27/100 [08:26<22:48, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  28%|██▊       | 28/100 [08:45<22:27, 18.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  29%|██▉       | 29/100 [09:03<22:07, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  30%|███       | 30/100 [09:22<21:48, 18.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  31%|███       | 31/100 [09:41<21:28, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  32%|███▏      | 32/100 [09:59<21:10, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  33%|███▎      | 33/100 [10:18<20:55, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  34%|███▍      | 34/100 [10:37<20:36, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  35%|███▌      | 35/100 [10:56<20:22, 18.81s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  36%|███▌      | 36/100 [11:15<20:04, 18.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  37%|███▋      | 37/100 [11:34<19:46, 18.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  38%|███▊      | 38/100 [11:52<19:27, 18.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  39%|███▉      | 39/100 [12:11<19:04, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  40%|████      | 40/100 [12:30<18:46, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  41%|████      | 41/100 [12:49<18:27, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  42%|████▏     | 42/100 [13:07<18:08, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  43%|████▎     | 43/100 [13:26<17:49, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  44%|████▍     | 44/100 [13:45<17:30, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  45%|████▌     | 45/100 [14:04<17:13, 18.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  46%|████▌     | 46/100 [14:22<16:54, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  47%|████▋     | 47/100 [14:41<16:37, 18.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  48%|████▊     | 48/100 [15:00<16:20, 18.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  49%|████▉     | 49/100 [15:19<16:01, 18.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  50%|█████     | 50/100 [15:38<15:42, 18.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  51%|█████     | 51/100 [15:57<15:24, 18.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  52%|█████▏    | 52/100 [16:16<15:05, 18.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  53%|█████▎    | 53/100 [16:34<14:41, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  54%|█████▍    | 54/100 [16:53<14:24, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  55%|█████▌    | 55/100 [17:12<14:07, 18.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  56%|█████▌    | 56/100 [17:31<13:48, 18.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  57%|█████▋    | 57/100 [17:50<13:29, 18.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  58%|█████▊    | 58/100 [18:08<13:10, 18.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  59%|█████▉    | 59/100 [18:27<12:49, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  60%|██████    | 60/100 [18:46<12:30, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  61%|██████    | 61/100 [19:05<12:11, 18.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  62%|██████▏   | 62/100 [19:23<11:52, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  63%|██████▎   | 63/100 [19:42<11:30, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  64%|██████▍   | 64/100 [20:00<11:11, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  65%|██████▌   | 65/100 [20:19<10:55, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  66%|██████▌   | 66/100 [20:38<10:37, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  67%|██████▋   | 67/100 [20:57<10:19, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  68%|██████▊   | 68/100 [21:16<10:01, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  69%|██████▉   | 69/100 [21:35<09:42, 18.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  70%|███████   | 70/100 [21:54<09:25, 18.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  71%|███████   | 71/100 [22:12<09:05, 18.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  72%|███████▏  | 72/100 [22:31<08:47, 18.83s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  73%|███████▎  | 73/100 [22:50<08:26, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  74%|███████▍  | 74/100 [23:08<08:05, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  75%|███████▌  | 75/100 [23:27<07:44, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  76%|███████▌  | 76/100 [23:45<07:27, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  77%|███████▋  | 77/100 [24:04<07:09, 18.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  78%|███████▊  | 78/100 [24:23<06:49, 18.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  79%|███████▉  | 79/100 [24:41<06:31, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  80%|████████  | 80/100 [25:00<06:12, 18.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  81%|████████  | 81/100 [25:18<05:53, 18.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  82%|████████▏ | 82/100 [25:37<05:34, 18.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  83%|████████▎ | 83/100 [25:56<05:17, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  84%|████████▍ | 84/100 [26:15<04:58, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  85%|████████▌ | 85/100 [26:33<04:39, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  86%|████████▌ | 86/100 [26:52<04:21, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  87%|████████▋ | 87/100 [27:11<04:02, 18.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  88%|████████▊ | 88/100 [27:29<03:44, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  89%|████████▉ | 89/100 [27:48<03:26, 18.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  90%|█████████ | 90/100 [28:07<03:07, 18.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  91%|█████████ | 91/100 [28:26<02:48, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  92%|█████████▏| 92/100 [28:44<02:29, 18.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  93%|█████████▎| 93/100 [29:03<02:10, 18.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  94%|█████████▍| 94/100 [29:22<01:52, 18.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  95%|█████████▌| 95/100 [29:41<01:33, 18.74s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  96%|█████████▌| 96/100 [29:59<01:14, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  97%|█████████▋| 97/100 [30:18<00:56, 18.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  98%|█████████▊| 98/100 [30:37<00:37, 18.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps:  99%|█████████▉| 99/100 [30:55<00:18, 18.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "Training Steps: 100%|██████████| 100/100 [31:14<00:00, 18.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Models: 100%|██████████| 2/2 [1:02:27<00:00, 1873.93s/it]\u001b[A\n",
            "Iterations: 100%|██████████| 2/2 [2:04:32<00:00, 3736.07s/it]  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "I = 2\n",
        "M = 2\n",
        "T = 100\n",
        "mu = 0.01\n",
        "nu = 0.5\n",
        "\n",
        "theta_ema = theta_init.copy()\n",
        "\n",
        "for i in tqdm(range(I), desc=\"Iterations\", position=0, leave=True):\n",
        "    for m in tqdm(range(M), desc=\"Models\", position=1, leave=True):\n",
        "        theta_m = gpt2_model.state_dict()\n",
        "\n",
        "        for t in tqdm(range(T), desc=\"Training Steps\", position=2, leave=True):\n",
        "            optimizer.zero_grad()\n",
        "            inputs = tokenizer(train_prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = {key: value.to(gpt2_model.device) for key, value in inputs.items()}\n",
        "            outputs = gpt2_model(**inputs)\n",
        "            ref_outputs = ref_model(**inputs)\n",
        "            generated_texts = tokenizer.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n",
        "\n",
        "            rewards = [compute_reward(prompt, text) for prompt, text in zip(train_prompts, generated_texts)]\n",
        "            rewards = torch.tensor(rewards).to(gpt2_model.device)\n",
        "            kl_divergence = evaluate_kl_divergence(outputs.logits, ref_outputs.logits)\n",
        "            kl_regularized_reward = rewards - nu * kl_divergence\n",
        "            loss = -kl_regularized_reward.mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            for key in theta_ema.keys():\n",
        "                theta_ema[key] = (1 - mu) * theta_ema[key] + mu * gpt2_model.state_dict()[key]\n",
        "\n",
        "        for key in theta_m.keys():\n",
        "            theta_m[key] = gpt2_model.state_dict()[key]\n",
        "\n",
        "    theta_slerp = slurp(theta_init, [theta_m] * M, lambd=0.5)\n",
        "    for key in theta_init.keys():\n",
        "        theta_init[key] = (1 - nu) * theta_init[key] + nu * theta_slerp[key]\n",
        "\n",
        "final_weights = {key: (1 - nu) * theta_sft[key] + nu * theta_slerp[key] for key in theta_sft.keys()}\n",
        "gpt2_model.load_state_dict(final_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c43b6713-a191-4566-aaf8-74d3d40af555",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T15:09:50.001471Z",
          "iopub.status.busy": "2024-08-05T15:09:50.000164Z",
          "iopub.status.idle": "2024-08-05T15:12:22.455088Z",
          "shell.execute_reply": "2024-08-05T15:12:22.453876Z",
          "shell.execute_reply.started": "2024-08-05T15:09:50.001424Z"
        },
        "tags": [],
        "id": "c43b6713-a191-4566-aaf8-74d3d40af555",
        "outputId": "492c93a3-5462-4a31-eee7-fc9e0fbb8f74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 100/100 [02:29<00:00,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Reward: 0.6583488184213638, Average KL Divergence: 439.11874591827393\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "test_df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])\n",
        "test_prompts = create_prompts(test_df, num_prompts=100)\n",
        "\n",
        "generated_responses = []\n",
        "rewards = []\n",
        "kl_divergences = []\n",
        "\n",
        "def pad_or_truncate(tensor, max_length):\n",
        "    if tensor.size(1) > max_length:\n",
        "        return tensor[:, :max_length]\n",
        "    elif tensor.size(1) < max_length:\n",
        "        pad_size = max_length - tensor.size(1)\n",
        "        return torch.nn.functional.pad(tensor, (0, 0, 0, pad_size), mode='constant', value=tokenizer.pad_token_id)\n",
        "    return tensor\n",
        "\n",
        "for prompt in tqdm(test_prompts, desc=\"Evaluation\", position=0, leave=True):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {key: value.to(gpt2_model.device) for key, value in inputs.items()}\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    output = gpt2_model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=input_ids.shape[-1] + 50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    generated_responses.append(generated_text)\n",
        "\n",
        "    reward = compute_reward(prompt, generated_text)\n",
        "    rewards.append(reward)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_outputs = ref_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    generated_inputs = tokenizer(generated_text, return_tensors=\"pt\", padding=True, truncation=True).to(gpt2_model.device)\n",
        "    generated_outputs = gpt2_model(**generated_inputs)\n",
        "\n",
        "    max_length = max(generated_outputs.logits.size(1), ref_outputs.logits.size(1))\n",
        "    generated_logits_padded = pad_or_truncate(generated_outputs.logits, max_length)\n",
        "    ref_logits_padded = pad_or_truncate(ref_outputs.logits, max_length)\n",
        "\n",
        "    kl_divergence = F.kl_div(generated_logits_padded.log_softmax(dim=-1), ref_logits_padded.softmax(dim=-1), reduction='batchmean')\n",
        "    kl_divergences.append(kl_divergence.mean().item())\n",
        "\n",
        "avg_reward = sum(rewards) / len(rewards)\n",
        "avg_kl_divergence = sum(kl_divergences) / len(kl_divergences)\n",
        "\n",
        "print(f\"Average Reward: {avg_reward}, Average KL Divergence: {avg_kl_divergence}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abafd1b-3ae2-4e71-968d-1c8ebb260456",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-05T15:15:12.807377Z",
          "iopub.status.busy": "2024-08-05T15:15:12.806300Z"
        },
        "id": "5abafd1b-3ae2-4e71-968d-1c8ebb260456"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_warp_with_mu(mu_value):\n",
        "    theta_init = gpt2_model.state_dict()\n",
        "    theta_sft = gpt2_model.state_dict()\n",
        "    theta_ema = theta_init.copy()\n",
        "\n",
        "    for i in range(I):\n",
        "        for m in range(M):\n",
        "            theta_m = gpt2_model.state_dict()\n",
        "            for t in range(T):\n",
        "                optimizer.zero_grad()\n",
        "                inputs = tokenizer(train_prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                inputs = {key: value.to(gpt2_model.device) for key, value in inputs.items()}\n",
        "                outputs = gpt2_model(**inputs)\n",
        "                ref_outputs = ref_model(**inputs)\n",
        "                generated_texts = tokenizer.batch_decode(outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n",
        "\n",
        "                rewards = [compute_reward(prompt, text) for prompt, text in zip(train_prompts, generated_texts)]\n",
        "                rewards = torch.tensor(rewards).to(gpt2_model.device)\n",
        "                kl_divergence = evaluate_kl_divergence(outputs.logits.float(), ref_outputs.logits.float())\n",
        "                kl_regularized_reward = rewards - nu * kl_divergence\n",
        "                loss = -kl_regularized_reward.mean()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                for key in theta_ema.keys():\n",
        "                    theta_ema[key] = (1 - mu_value) * theta_ema[key] + mu_value * gpt2_model.state_dict()[key]\n",
        "            for key in theta_m.keys():\n",
        "                theta_m[key] = gpt2_model.state_dict()[key]\n",
        "        theta_slerp = slurp(theta_init, [theta_m] * M, lambd=0.5)\n",
        "        for key in theta_init.keys():\n",
        "            theta_init[key] = (1 - nu) * theta_init[key] + nu * theta_slerp[key]\n",
        "\n",
        "    final_weights = {key: (1 - nu) * theta_sft[key] + nu * theta_slerp[key] for key in theta_sft.keys()}\n",
        "    gpt2_model.load_state_dict(final_weights)\n",
        "\n",
        "    generated_responses = []\n",
        "    rewards = []\n",
        "    kl_divergences = []\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = {key: value.to(gpt2_model.device) for key, value in inputs.items()}\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        output = gpt2_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=input_ids.shape[-1] + 50,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        generated_responses.append(generated_text)\n",
        "\n",
        "        reward = compute_reward(prompt, generated_text)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        ref_outputs = ref_model(input_ids)\n",
        "        generated_inputs = tokenizer(generated_text, return_tensors=\"pt\", padding=True, truncation=True).to(gpt2_model.device)\n",
        "        generated_outputs = gpt2_model(**generated_inputs)\n",
        "\n",
        "        max_length = max(generated_outputs.logits.size(1), ref_outputs.logits.size(1))\n",
        "        generated_logits_padded = pad_or_truncate(generated_outputs.logits, max_length)\n",
        "        ref_logits_padded = pad_or_truncate(ref_outputs.logits, max_length)\n",
        "\n",
        "        kl_divergence = F.kl_div(generated_logits_padded.log_softmax(dim=-1), ref_logits_padded.softmax(dim=-1), reduction='batchmean')\n",
        "        kl_divergences.append(kl_divergence.mean().item())\n",
        "\n",
        "    avg_reward = sum(rewards) / len(rewards)\n",
        "    avg_kl_divergence = sum(kl_divergences) / len(kl_divergences)\n",
        "\n",
        "    return avg_reward, avg_kl_divergence\n",
        "\n",
        "mu_values = [0.01, 0.05, 0.1]\n",
        "results = []\n",
        "\n",
        "for mu in mu_values:\n",
        "    avg_reward, avg_kl_divergence = run_warp_with_mu(mu)\n",
        "    results.append((mu, avg_reward, avg_kl_divergence))\n",
        "    print(f\"mu: {mu}, Average Reward: {avg_reward}, Average KL Divergence: {avg_kl_divergence}\")\n",
        "\n",
        "mus, rewards, kls = zip(*results)\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:blue'\n",
        "ax1.set_xlabel('mu')\n",
        "ax1.set_ylabel('Average Reward', color=color)\n",
        "ax1.plot(mus, rewards, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:red'\n",
        "ax2.set_ylabel('Average KL Divergence', color=color)\n",
        "ax2.plot(mus, kls, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title('Average Reward and KL Divergence vs mu')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Вывод"
      ],
      "metadata": {
        "id": "8g3PQMW0hfyt"
      },
      "id": "8g3PQMW0hfyt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Цель**\n",
        "\n",
        "Цель проекта заключалась в реализации алгоритма WARP (Weighted Approximate Rank Pairwise) и его оценке с использованием модели наград, обученной на датасете IMDb. Мы стремились измерить среднюю награду и дивергенцию Кульбака-Лейблера (KL), а также визуализировать влияние различных гиперпараметров на производительность модели.\n",
        "\n",
        "**Шаги реализации**\n",
        "\n",
        "1.Подготовка данных:\n",
        "\n",
        "Загрузили датасет IMDb.\n",
        "Создали пары положительных и отрицательных комментариев из обучающей подвыборки.\n",
        "Обучили модель наград, используя distilbert-base-cased, на этих парах.\n",
        "Реализация алгоритма WARP:\n",
        "\n",
        "Инициализировали модель GPT-2 (gpt2-imdb) и модель наград.\n",
        "Реализовали алгоритм WARP с итерациями по разным моделям и шагам обучения.\n",
        "Использовали SLERP (Spherical Linear Interpolation) для объединения весов моделей.\n",
        "\n",
        "2.Оценка:\n",
        "\n",
        "Сгенерировали 100 запросов из тестовой подвыборки датасета IMDb.\n",
        "Измерили среднюю награду и дивергенцию KL для сгенерированных выводов.\n",
        "Реализовали дополнение и усечение для обеспечения совпадения размеров тензоров при вычислении дивергенции KL.\n",
        "\n",
        "3.Настройка гиперпараметров:\n",
        "\n",
        "Выбрали mu как гиперпараметр для изменения.\n",
        "Запустили алгоритм WARP с тремя разными значениями mu (0.01, 0.05 и 0.1).\n",
        "\n",
        "**Результаты**\n",
        "\n",
        "Средняя награда и дивергенция KL:\n",
        "\n",
        "При mu=0.01: Средняя награда: 0.658, Средняя дивергенция KL: 439.119\n",
        "При mu=0.05: (Не удалось получить в указанные сроки ввиду ограничения вычислительных мощностей)\n",
        "При mu=0.1: (Не удалось получить в указанные сроки ввиду ограничения вычислительных мощностей)\n",
        "\n",
        "\n",
        "Проблемы и возможные причины\n",
        "Высокая дивергенция KL:\n",
        "\n",
        "Одной из основных проблем была высокая дивергенция KL, что указывало на значительное расхождение между выходными данными модели и референтной модели.\n",
        "\n",
        "\n",
        "**Выводы**\n",
        "\n",
        "Алгоритм WARP успешно изменяет веса модели, но высокая дивергенция KL указывает на необходимость дальнейшей настройки и оценки."
      ],
      "metadata": {
        "id": "CKqdqH6ehjx6"
      },
      "id": "CKqdqH6ehjx6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DataSphere Kernel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
